seed: 1234

# Number of evolutionary generations (outer loop).
generations: 10

# Population size per generation.
population_size: 8

# Number of elites carried over between generations.
elite_size: 4

# Number of initial LLM-generated candidates.
init_llm: 8

# Multi-fidelity budget (in training steps).
f0_only: false
# High-fidelity training budget for F1.
# Option 1 (legacy): specify the total number of training *steps* directly.
f1_steps: 0
# Option 2: specify epochs and instances per epoch. When both are > 0,
# they are used to derive the total number of training steps as:
#   total_steps = hf_epochs * ceil(hf_instances_per_epoch / train_batch_size)
hf_epochs: 20
hf_instances_per_epoch: 100000
f2_steps: 0    # set >0 to enable F2
f3_enabled: false
# LLM repair retries when a candidate fails compile/dynamic/preference gates.
# This controls the "attempting repair (repair_round=...)" log lines.
max_repair_rounds: 4

# Directed repair (CEGIS-style) when dynamic gates fail.
directed_repair_enabled: true
# Per generation budgets (keep bounded to avoid repair loops).
directed_repair_max_parents_per_generation: 4
directed_repair_children_per_strategy: 1

# Early-eval horizon for baseline/candidates, expressed in epochs/instances.
# If both are > 0, we compute:
#   early_eval_steps = early_eval_epochs * ceil(early_eval_instances_per_epoch / train_batch_size)
# Otherwise we fall back to a legacy 100-step early evaluation.
early_eval_epochs: 5
early_eval_instances_per_epoch: 100000

# Training / evaluation config for TSP.
# Use TSP20 for both training and evaluation so that each candidate's
# fitness reflects full training behaviour on the canonical small-scale
# benchmark.
problem: tsp
train_problem_size: 100
valid_problem_sizes: [100]
train_batch_size: 64
pomo_size: 100
learning_rate: 0.0003
weight_decay: 0.000001
alpha: 0.05
device: cuda
num_validation_episodes: 10000
validation_batch_size: 64
generalization_penalty_weight: 1.0
# Cross-size aggregation of evaluation objectives.
# - legacy: train_size objective + (max(valid)-train_size) penalty (previous behavior)
# - cvar: CVaR over sizes (worst-tail average; CO-aligned robustness)
# - worst: worst-case size objective
# - mean: mean over sizes
size_aggregation: cvar
size_cvar_alpha: 0.2
baseline_epoch_violation_weight: 0.0
pref_semantic_gate_enabled: true
pref_semantic_trials: 6
pref_semantic_batch_size: 128
pref_semantic_min_pass_rate: 0.5
pref_semantic_swap_tolerance: 0.01
pref_semantic_gap_min_ratio: 0.5

# CO-alignment gates (enabled by default).
co_gate_enabled: true
co_sensitivity_min_abs_delta: 0.001
co_sensitivity_min_rel_delta: 0.01
co_invariance_max_abs_delta: 0.001
co_invariance_max_rel_delta: 0.01

# Gate thresholds for dynamic checks.
grad_norm_max: 10.0
loss_soft_min: -5.0
loss_soft_max: 5.0
# Use a second "hidden" dynamic gate suite (different sampling) for acceptance.
hidden_dynamic_gates_enabled: true
# Drop-and-resample retries for *structural* dynamic-gate failures (e.g. missing
# batch keys / expects / dependencies). This controls the
# "dropping candidate and resampling (resample_round=...)" log lines.
max_resample_rounds: 4

# Diversity / novelty configuration.
novelty_behavior_deltas: [-10, -5, -2, -1, 0, 1, 2, 5, 10]
novelty_k: 5
novelty_ops_weight: 1.0
novelty_hparam_weight: 0.5
novelty_behavior_weight: 1.0
novelty_thought_weight: 0.25
diversity_archive_size: 32
burn_in_objectives_auto: true

# Whitelist of allowed operators in free-form losses.
operator_whitelist:
  - logsigmoid
  - softplus
  - sigmoid
  - exp
  - log
  - tanh
  - relu
  - clamp
  - normalize
  - zscore
  - rank_gap

# Paths to prompt templates (relative to repo root).
prompts:
  generation: prompts/free_loss_generation.txt
  crossover: prompts/free_loss_crossover.txt
  e2: prompts/free_loss_e2.txt
  mutation: prompts/free_loss_mutation.txt
  m2: prompts/free_loss_m2.txt
  m3: prompts/free_loss_m3.txt
  repair: prompts/free_loss_repair.txt
  directed_repair: prompts/free_loss_directed_repair.txt
  expects_repair: prompts/free_loss_expects_repair.txt

# Output directory pattern for discovery runs.
output_root: runs/free_loss_discovery
